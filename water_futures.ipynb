{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Futures approach to the Battle \n",
    "Spiegazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the workspace\n",
    "import eval\n",
    "from eval.evaluator import WaterFuturesEvaluator\n",
    "from eval.dashboard import run_dashboard\n",
    "\n",
    "# prepare the evaluator\n",
    "wfe = WaterFuturesEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1\n",
    "Iteration one is where we try and test as many models with as many tuning as we can. Then, we will select only a handful of them and the best tuning for each family. \n",
    "Also we try which is the best reconciliation technique between those available.\n",
    "So the dataset is divided in *training* where we train and validate the models and the strategies and *test* where the models are tested on and the selected technique will produce the final forecast for the competition.\n",
    "Following iterations will have only the second part as the training part is not really necessary anymore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the evaluator for the next iteration\n",
    "wfe.next_iter()\n",
    "# Collect all the models and the settings that we are considering\n",
    "import models\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the models\n",
    "from models.benchmark import RollingAverageWeek, AutoRollingAverageWeek\n",
    "from preprocessing.impute_and_fill import FillZero, FillAvgWeek\n",
    "\n",
    "previous_week = {\n",
    "    'name': 'PrevWeek',\n",
    "    'model': RollingAverageWeek(1),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillZero()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "previous_week_v2 = {\n",
    "    'name': 'PrevWeek_v2',\n",
    "    'model': RollingAverageWeek(1),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillAvgWeek()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "average_week = {\n",
    "    'name': 'AvgWeek',\n",
    "    'model': RollingAverageWeek(None),\n",
    "    'preprocessing': {\n",
    "        'demand': [],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "rolling_average_2 = {\n",
    "    'name': 'RollingAverage_2',\n",
    "    'model': RollingAverageWeek(2),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillZero()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "rolling_average_4 = {\n",
    "    'name': 'RollingAverage_4',\n",
    "    'model': RollingAverageWeek(4),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillZero()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "rolling_average_8 = {\n",
    "    'name': 'RollingAverage_8',\n",
    "    'model': RollingAverageWeek(8),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillZero()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "auto_rollaw = {\n",
    "    'name': 'AutoRollingAverage',\n",
    "    'model': AutoRollingAverageWeek(),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillAvgWeek()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': False\n",
    "}\n",
    "\n",
    "models_configs = [\n",
    "    previous_week,\n",
    "    previous_week_v2,\n",
    "    average_week,\n",
    "    rolling_average_2,\n",
    "    rolling_average_4,\n",
    "    rolling_average_8,\n",
    "    auto_rollaw\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Roll Ave \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.exp_rolling_average_week import ExpWeightedRollingWeek\n",
    "\n",
    "exp_rolling_average_2 = {\n",
    "    'name': 'ExpRollingAverage_2',\n",
    "    'model': ExpWeightedRollingWeek(2),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillAvgWeek()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "exp_rolling_average_4 = {\n",
    "    'name': 'ExpRollingAverage_4',\n",
    "    'model': ExpWeightedRollingWeek(4),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillAvgWeek()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "exp_rolling_average_8 = {\n",
    "    'name': 'ExpRollingAverage_8',\n",
    "    'model': ExpWeightedRollingWeek(8),\n",
    "    'preprocessing': {\n",
    "        'demand': [FillAvgWeek()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "models_configs += [\n",
    "    exp_rolling_average_2,\n",
    "    exp_rolling_average_4,\n",
    "    exp_rolling_average_8\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern regression\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pattern_regression import PatternRegression, PatternRegressionDaily\n",
    "from preprocessing.simple_transforms import Logarithm\n",
    "from preprocessing.weather_feature_engineering import RealFeel, DewPoint, WindChill\n",
    "\n",
    "pattern_regression = {\n",
    "    'name': f'PatternRegression',\n",
    "    'model': PatternRegression(),\n",
    "    'preprocessing': {\n",
    "        'demand': [Logarithm()],\n",
    "        'weather': [RealFeel(), DewPoint(), WindChill()]\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "pattern_regression_daily = {\n",
    "    'name': f'PatternRegressionDaily',\n",
    "    'model': PatternRegressionDaily(),\n",
    "    'preprocessing': {\n",
    "        'demand': [Logarithm()],\n",
    "        'weather': [RealFeel(), DewPoint(), WindChill()]\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "models_configs += [\n",
    "    pattern_regression,\n",
    "    pattern_regression_daily\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.fbprophet import Fbprophet\n",
    "\n",
    "prophet = {\n",
    "    'name': 'FbProphet',\n",
    "    'model': Fbprophet(),\n",
    "    'preprocessing': {\n",
    "        'demand': [],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': True\n",
    "}\n",
    "\n",
    "models_configs += [\n",
    "    prophet\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models \n",
    "### LGBM\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LGBM import LGBMrobust, LGBMsimple\n",
    "from preprocessing.advanced_transforms import LGBM_demand_features, LGBM_impute_nan_demand\n",
    "from preprocessing.advanced_transforms import LGBM_impute_nan_weather, LGBM_weather_features\n",
    "from preprocessing.advanced_transforms import  LGBM_prepare_test_dfs\n",
    "\n",
    "# No hyperparameter tuning for all parameters\n",
    "lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'num_leaves': 32,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.6,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq':10,\n",
    "        'verbose': -1\n",
    "}\n",
    "\n",
    "lgbm_simple = {\n",
    "    'name': 'LGBMsimple',\n",
    "    'model': LGBMsimple(lgb_params = lgb_params),\n",
    "    'preprocessing': {\n",
    "        'demand': [Logarithm(), LGBM_impute_nan_demand(), LGBM_demand_features(no_last_week=1)],\n",
    "        'weather': [LGBM_impute_nan_weather(), LGBM_weather_features()],\n",
    "        'prepare_test_dfs': [LGBM_prepare_test_dfs()]\n",
    "    },\n",
    "    'deterministic': False\n",
    "}\n",
    "lgbm_robust = {\n",
    "    'name': 'LGBMrobust',\n",
    "    'model': LGBMrobust(lgb_params = lgb_params),\n",
    "    'preprocessing': {\n",
    "        'demand': [Logarithm(), LGBM_impute_nan_demand(), LGBM_demand_features(no_last_week=1)],\n",
    "        'weather': [LGBM_impute_nan_weather(), LGBM_weather_features()],\n",
    "        'prepare_test_dfs': [LGBM_prepare_test_dfs()]\n",
    "    },\n",
    "    'deterministic': False\n",
    "}\n",
    "lgbm_simple_with_last_week = {\n",
    "    'name': 'LGBMsimple_with_last week',\n",
    "    'model': LGBMsimple(lgb_params = lgb_params),\n",
    "    'preprocessing': {\n",
    "        'demand': [Logarithm(), LGBM_impute_nan_demand(), LGBM_demand_features(no_last_week=0)],\n",
    "        'weather': [LGBM_impute_nan_weather(), LGBM_weather_features()],\n",
    "        'prepare_test_dfs': [LGBM_prepare_test_dfs()]\n",
    "    },\n",
    "    'deterministic': False\n",
    "}\n",
    "\n",
    "models_configs += [\n",
    "    lgbm_simple,\n",
    "    lgbm_robust,\n",
    "    lgbm_simple_with_last_week\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LGBM import XGBMsimple\n",
    "\n",
    "xgb_params = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'objective':'reg:squarederror',\n",
    "    'min_child_weight':10,\n",
    "    'silent':1\n",
    "}\n",
    "\n",
    "xgbm_simple = {\n",
    "    'name': 'XGBMsimple',\n",
    "    'model': XGBMsimple(xgb_params = xgb_params),\n",
    "    'preprocessing': {\n",
    "        'demand': [Logarithm(), LGBM_impute_nan_demand(), LGBM_demand_features(no_last_week=0)],\n",
    "        'weather': [LGBM_impute_nan_weather(), LGBM_weather_features()],\n",
    "        'prepare_test_dfs': [LGBM_prepare_test_dfs()]\n",
    "    },\n",
    "    'deterministic': False\n",
    "}\n",
    "\n",
    "models_configs += [\n",
    "    xgbm_simple\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSMIX\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.TSMix import TSMix\n",
    "\n",
    "tsmix = {\n",
    "    'name': 'TSMix',\n",
    "    'model': TSMix(train_epochs=50, dropout=0.8),\n",
    "    'preprocessing': {\n",
    "        'demand': [Logarithm(), LGBM_impute_nan_demand()],\n",
    "        'weather': []\n",
    "    },\n",
    "    'deterministic': False\n",
    "}\n",
    "\n",
    "models_configs += [\n",
    "    tsmix\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.wavenet import WaveNetModel, WaveNet_prepare_test_dfs, cfg\n",
    "\n",
    "#cfg['device'] = 'cuda' # if you have a compatible NVIDIA GPU\n",
    "cfg['device'] = 'mps:0' # if you have Metal acceleration on your Mac (https://developer.apple.com/metal/pytorch/)\n",
    "#cfg['device'] = 'cpu' # for every other machine without GPU acceleration\n",
    "\n",
    "wavenet = {\n",
    "    'name': 'WaveNet',\n",
    "    'model': WaveNetModel(cfg),\n",
    "    'preprocessing': {\n",
    "        'demand': [],\n",
    "        'weather': [],\n",
    "        'prepare_test_dfs': [WaveNet_prepare_test_dfs()]\n",
    "    },\n",
    "    'deterministic': False\n",
    "}\n",
    "\n",
    "models_configs += [\n",
    "    wavenet\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can run the training of all these models and see how they perform\n",
    "wfe.curr_phase='train'\n",
    "wfe.n_train_seeds = 1\n",
    "for config in models_configs:\n",
    "    wfe.add_model(config)\n",
    "\n",
    "# See how all the models perform \n",
    "# use the water_futures_dash.py script to run the dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After studying the dashboard we selected the models that it made more sense to combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models_sett = [auto_rollaw,\n",
    "                   pattern_regression,\n",
    "                   lgbm_simple,\n",
    "                   lgbm_robust,\n",
    "                   xgbm_simple,\n",
    "                   lgbm_simple_with_last_week,\n",
    "                   wavenet\n",
    "                   ]\n",
    "\n",
    "wfe.selected_models = [config['name'] for config in selected_models_sett]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Ensembling study\n",
    "now let's see how the ensembling strategies perfrom on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how the different strategies to reconcile the ensemble work\n",
    "from eval.strategies.best_on_history import BestOnLastNW, BestOnTest\n",
    "\n",
    "strategies = {}\n",
    "strategies['best_on_last'] = BestOnLastNW(1)\n",
    "strategies['best_on_last_2'] = BestOnLastNW(2)\n",
    "strategies['best_on_last_3'] = BestOnLastNW(3)\n",
    "strategies['best_on_test'] = BestOnTest() # like bestonlastNw(4)\n",
    "\n",
    "from eval.strategies.weighted_averages import WeightedAverage\n",
    "import numpy as np\n",
    "\n",
    "top5 = [lgbm_robust, lgbm_simple, xgbm_simple, lgbm_simple_with_last_week, wavenet]\n",
    "\n",
    "strategies['avg_top5'] = WeightedAverage([config['name'] for config in top5], np.ones(len(top5))/len(top5))\n",
    "\n",
    "top3 = [lgbm_robust, lgbm_simple, wavenet]\n",
    "strategies['avg_top3'] = WeightedAverage([config['name'] for config in top3], \n",
    "                                         np.ones(len(top3))/len(top3))\n",
    "\n",
    "strategies['lgbm_wavenet'] = WeightedAverage([lgbm_robust['name'], wavenet['name']], \n",
    "                                             np.array([0.5, 0.5]))\n",
    "\n",
    "# As suggested by Pansos, all the gbm models should count as one model, so 1/3 to rollaverage, 1/3 to gbms 1/3 to wavenet,\n",
    "# and then 1/4 to each of the gbms \n",
    "strategies['gbms_wavenet_arw'] = WeightedAverage([config['name'] for config in selected_models_sett],\n",
    "                                                  np.array([1/3, 0, 1/3/4, 1/3/4, 1/3/4, 1/3/4, 1/3]))\n",
    "\n",
    "# Last strategy is the weighted average where the weights are the ratio between mean and std on the training dataset\n",
    "from Utils.process_results import extract_from\n",
    "from eval.data_loading_helpers import DMAS_NAMES\n",
    "\n",
    "weights = {}\n",
    "for dma in DMAS_NAMES:\n",
    "    dma_pis = [extract_from(wfe.results[model], 1, 'train', 'performance_indicators') for model in wfe.selected_models]\n",
    "    dmas_pi_weight = np.array([(\n",
    "            dma_pis[i].groupby('DMA').mean().loc[dma,'PI3']\n",
    "                                )/(\n",
    "            dma_pis[i].groupby('DMA').std().loc[dma,'PI3']\n",
    "                                ) for i in range(len(dma_pis)) ])\n",
    "    dmas_pi_weight[1] = 0 # I don't want to consider the pattern regression\n",
    "    weights[dma] = dmas_pi_weight/dmas_pi_weight.sum()\n",
    "\n",
    "strategies['wavg_train'] = WeightedAverage([model for model in wfe.selected_models], \n",
    "                                           weights)\n",
    "\n",
    "for strategy in strategies:\n",
    "    wfe.add_strategy(strategy, strategies[strategy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " here we look at the strategies in thesame dashboard and see how they perform\n",
    " use the water_futures_dash.py script to run the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the strategy to go\n",
    "wfe.selected_strategy = 'avg_top5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the selected models on the test \n",
    "wfe.n_test_seeds = 3\n",
    "wfe.forecast_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can run again the dashboard to see the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BoN2024_dev_py3116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
